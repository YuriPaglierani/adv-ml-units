{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *Challenge 2*: *Discovering Complexity in Neural Networks*\n",
        "\n",
        "Advanced Topics in Machine Learning -- Fall 2023, UniTS\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/notebooks/AdvML_Challenge_2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ],
      "metadata": {
        "id": "s9Qlhe8rSlQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Notebook we will perform an automatic analysis of complexity of a CNN over the Cifar10 dataset, in order to understand how non-linear is our feature representation (I choose this challenge as second project instead the the Challenge 2)"
      ],
      "metadata": {
        "id": "PhP-0bkOSorc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Idea\n",
        "We'll analyze a naive CNN architecture: Convolution -> BN -> ReLU -> MaxPool -> ... -> Linear -> Softmax\n",
        "\n",
        "We will boost a bit this cute model with pruning technique.\n",
        "\n",
        "Now, we want to set a trainable non-linearity $R_{\\beta}(x) = ReLU(x) - \\beta \\cdot ReLU(-x)$, if we take $\\beta = (1 - \\alpha)$ we obtain\n",
        "\n",
        "$R_{\\alpha}(x) = ReLU(x) - (1-\\alpha)\\cdot ReLU(-x)$\n",
        "\n",
        "If we observe that $x = ReLU(x) - ReLU(-x)$ we finally obtain:\n",
        "\n",
        "$$R_{\\alpha}(x) = x + \\alpha \\cdot ReLU(-x)$$\n",
        "\n",
        "Where $\\alpha \\in [0, 1]$\n",
        "\n",
        "*Note*: you can also interpret the action of $R_\\alpha(x)$ as a residual connection!\n",
        "\n",
        "*Note*: we'll use a different $\\alpha$ for each non linearity"
      ],
      "metadata": {
        "id": "ok6hLP30SsSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There could be different way to model $\\alpha$, Professor Anselmi suggested to simply penalize the final loss with the L1 Norm of the alphas, instead I'll use $\\alpha_t = f((f(\\gamma)-0.5)\\nu_t)$, with $\\gamma$ trainable, $f(x) = 1/(1+e^{-x})$, and $\\nu_t$ increasing in time, so in the end the model will automatically select if the $\\alpha$ is relevant or not.\n",
        "\n",
        "You can find the same updating strategy in this paper: https://appliednetsci.springeropen.com/articles/10.1007/s41109-023-00542-x"
      ],
      "metadata": {
        "id": "S0zrH6iQSwvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "VcnPdTv3GwR-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GammaAlphaCompute(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Computes and tracks the alpha value based on a learnable gamma parameter.\n",
        "\n",
        "    This module is responsible for computing the alpha value used in the CustomNonLinearity.\n",
        "    It maintains a history of alpha and gamma values during training.\n",
        "\n",
        "    Attributes:\n",
        "        gamma (nn.Parameter): Learnable parameter used to compute alpha.\n",
        "        nu (torch.Tensor): A buffer that increases during training.\n",
        "        name (str): Identifier for the instance.\n",
        "        alpha_history (List[float]): History of computed alpha values during training.\n",
        "        gamma_history (List[float]): History of gamma values during training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name: str):\n",
        "        \"\"\"\n",
        "        Initializes the GammaAlphaCompute module.\n",
        "\n",
        "        Args:\n",
        "            name (str): Identifier for this instance.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.tensor(0.0))\n",
        "        self.register_buffer('nu', torch.tensor(0.0))\n",
        "        self.name = name\n",
        "        self.alpha_history: List[float] = []\n",
        "        self.gamma_history: List[float] = []\n",
        "\n",
        "    def compute_alpha(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes the alpha value based on the current gamma and nu.\n",
        "\n",
        "        If in training mode, this method also updates nu and records the\n",
        "        current alpha and gamma values in their respective histories.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The computed alpha value.\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "            self.nu = self.nu + 0.01  # Increase nu during training\n",
        "\n",
        "        f_gamma = torch.sigmoid(self.gamma)\n",
        "        alpha = torch.sigmoid((f_gamma - 0.5) * self.nu)\n",
        "\n",
        "        if self.training:\n",
        "            self.alpha_history.append(alpha.item())\n",
        "            self.gamma_history.append(self.gamma.item())\n",
        "\n",
        "        return alpha\n",
        "\n",
        "class CustomNonLinearity(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a custom non-linearity function using the computed alpha value.\n",
        "\n",
        "    This module applies a non-linear transformation to the input using an alpha\n",
        "    value computed by a GammaAlphaCompute instance.\n",
        "\n",
        "    Attributes:\n",
        "        ga_compute (GammaAlphaCompute): Instance to compute the alpha value.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name: str):\n",
        "        \"\"\"\n",
        "        Initializes the CustomNonLinearity module.\n",
        "\n",
        "        Args:\n",
        "            name (str): Identifier for this instance, passed to GammaAlphaCompute.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.ga_compute = GammaAlphaCompute(name)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Applies the custom non-linearity to the input tensor.\n",
        "\n",
        "        The non-linearity is defined as: f(x) = x + alpha * ReLU(-x)\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The result of applying the non-linearity to the input.\n",
        "        \"\"\"\n",
        "        alpha = self.ga_compute.compute_alpha()\n",
        "        return x + alpha * F.relu(-x)"
      ],
      "metadata": {
        "id": "Iy7zgPhBhx_x"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicPruningCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A Convolutional Neural Network with dynamic pruning capabilities.\n",
        "\n",
        "    This CNN can dynamically remove layers during training based on their importance,\n",
        "    as determined by the alpha values of CustomNonLinearity layers.\n",
        "\n",
        "    Attributes:\n",
        "        layers (nn.ModuleList): List of CNN layers.\n",
        "        input_channels (int): Number of input channels.\n",
        "        hidden_channels (int): Number of hidden channels in CNN layers.\n",
        "        num_classes (int): Number of output classes.\n",
        "        output_size (int): Size of the flattened output before the final FC layer.\n",
        "        fc (nn.Linear): Final fully connected layer.\n",
        "        softmax (nn.Softmax): Softmax activation for output.\n",
        "        gamma_history (dict): History of gamma values for each layer.\n",
        "        alpha_history (dict): History of alpha values for each layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, initial_layers: int = 3, input_channels: int = 3,\n",
        "                 hidden_channels: int = 64, num_classes: int = 10):\n",
        "        \"\"\"\n",
        "        Initializes the DynamicPruningCNN.\n",
        "\n",
        "        Args:\n",
        "            initial_layers (int): Number of initial CNN layers.\n",
        "            input_channels (int): Number of input channels.\n",
        "            hidden_channels (int): Number of hidden channels in CNN layers.\n",
        "            num_classes (int): Number of output classes.\n",
        "        \"\"\"\n",
        "        super(DynamicPruningCNN, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        for i in range(initial_layers):\n",
        "            in_channels = input_channels if i == 0 else hidden_channels\n",
        "            self.layers.append(self._create_cnn_block(in_channels, hidden_channels, f\"CNN{i+1}\"))\n",
        "\n",
        "        self.output_size = self._get_output_size(initial_layers)\n",
        "        self.fc = nn.Linear(self.output_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self.gamma_history = {f\"CNN{i+1}\": [] for i in range(initial_layers)}\n",
        "        self.alpha_history = {f\"CNN{i+1}\": [] for i in range(initial_layers)}\n",
        "\n",
        "    def _create_cnn_block(self, in_channels: int, out_channels: int, name: str) -> nn.Sequential:\n",
        "        \"\"\"\n",
        "        Creates a CNN block consisting of Conv2d, BatchNorm2d, and CustomNonLinearity.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels.\n",
        "            out_channels (int): Number of output channels.\n",
        "            name (str): Name for the CustomNonLinearity layer.\n",
        "\n",
        "        Returns:\n",
        "            nn.Sequential: A CNN block.\n",
        "        \"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            CustomNonLinearity(name)\n",
        "        )\n",
        "\n",
        "    def _get_output_size(self, num_layers: int) -> int:\n",
        "        \"\"\"\n",
        "        Calculates the output size after applying all CNN layers and max pooling.\n",
        "\n",
        "        Args:\n",
        "            num_layers (int): Number of CNN layers.\n",
        "\n",
        "        Returns:\n",
        "            int: The size of the flattened output.\n",
        "        \"\"\"\n",
        "        x = torch.randn(1, self.input_channels, 32, 32)  # CIFAR-10 input size\n",
        "        for _ in range(num_layers):\n",
        "            x = F.max_pool2d(x, 2)\n",
        "        return x.numel() * self.hidden_channels // x.shape[1]\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after passing through the network.\n",
        "        \"\"\"\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            x = F.max_pool2d(x, 2)\n",
        "\n",
        "            layer_name = f\"CNN{i+1}\"\n",
        "            self.gamma_history[layer_name].append(layer[-1].ga_compute.gamma.item())\n",
        "            self.alpha_history[layer_name].append(layer[-1].ga_compute.compute_alpha().item())\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "    def prune_and_rebuild(self, alpha_threshold: float = 0.01) -> Tuple['DynamicPruningCNN', Optional[int]]:\n",
        "        \"\"\"\n",
        "        Prunes the network by removing a layer with alpha value below the threshold.\n",
        "\n",
        "        If a layer is pruned, a new model is created with the remaining layers,\n",
        "        and the weights are transferred from the old model to the new one.\n",
        "\n",
        "        Args:\n",
        "            alpha_threshold (float): Threshold for pruning decision.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[DynamicPruningCNN, Optional[int]]: A tuple containing the new (or same) model\n",
        "            and the index of the pruned layer (or None if no pruning occurred).\n",
        "        \"\"\"\n",
        "        pruned_layer_index = next((i for i, layer in enumerate(self.layers)\n",
        "                                   if layer[-1].ga_compute.compute_alpha() < alpha_threshold), None)\n",
        "\n",
        "        if pruned_layer_index is not None:\n",
        "            new_model = DynamicPruningCNN(len(self.layers) - 1,\n",
        "                                          self.input_channels,\n",
        "                                          self.hidden_channels,\n",
        "                                          self.num_classes)\n",
        "\n",
        "            # This part copies weights and history from the old model to the new submodel\n",
        "            with torch.no_grad():\n",
        "                for i in range(pruned_layer_index):\n",
        "                    new_model.layers[i].load_state_dict(self.layers[i].state_dict())\n",
        "                    new_model.gamma_history[f\"CNN{i+1}\"] = self.gamma_history[f\"CNN{i+1}\"]\n",
        "                    new_model.alpha_history[f\"CNN{i+1}\"] = self.alpha_history[f\"CNN{i+1}\"]\n",
        "\n",
        "                for i in range(pruned_layer_index, len(new_model.layers)):\n",
        "                    new_model.layers[i].load_state_dict(self.layers[i+1].state_dict())\n",
        "                    new_model.gamma_history[f\"CNN{i+1}\"] = self.gamma_history[f\"CNN{i+2}\"]\n",
        "                    new_model.alpha_history[f\"CNN{i+1}\"] = self.alpha_history[f\"CNN{i+2}\"]\n",
        "\n",
        "                # Preserve the history of the pruned layer\n",
        "                pruned_layer_name = f\"CNN{pruned_layer_index+1}\"\n",
        "                new_model.gamma_history[pruned_layer_name] = self.gamma_history[pruned_layer_name]\n",
        "                new_model.alpha_history[pruned_layer_name] = self.alpha_history[pruned_layer_name]\n",
        "\n",
        "                # Adjust the FC layer\n",
        "                if new_model.output_size != self.output_size:\n",
        "                    new_fc = nn.Linear(new_model.output_size, self.num_classes)\n",
        "                    new_fc.weight.data[:, :self.output_size] = self.fc.weight.data\n",
        "                    new_fc.bias.data = self.fc.bias.data\n",
        "                    new_model.fc = new_fc\n",
        "                else:\n",
        "                    new_model.fc.load_state_dict(self.fc.state_dict())\n",
        "\n",
        "            # Freeze all layers except those after the pruned layer\n",
        "            for i, layer in enumerate(new_model.layers):\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = (i >= pruned_layer_index)\n",
        "\n",
        "            return new_model, pruned_layer_index\n",
        "\n",
        "        return self, None\n",
        "\n",
        "    def unfreeze_all_layers(self) -> None:\n",
        "        \"\"\"\n",
        "        Unfreezes all layers in the network, making them trainable.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True"
      ],
      "metadata": {
        "id": "35_Mt55DiAyW"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given dataloader.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to evaluate.\n",
        "        dataloader (DataLoader): The dataloader containing the evaluation data.\n",
        "        criterion (nn.Module): The loss function.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: A tuple containing the average loss and accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return running_loss / len(dataloader), 100 * correct / total\n",
        "\n",
        "def train_with_dynamic_pruning(model: DynamicPruningCNN,\n",
        "                               trainloader: DataLoader,\n",
        "                               valloader: DataLoader,\n",
        "                               criterion: nn.Module,\n",
        "                               optimizer: optim.Optimizer,\n",
        "                               num_epochs: int,\n",
        "                               alpha_threshold: float = 0.01) -> Tuple[DynamicPruningCNN, List[float], List[float], List[float], List[float], List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    Trains the model with dynamic pruning.\n",
        "\n",
        "    Args:\n",
        "        model (DynamicPruningCNN): The model to train.\n",
        "        trainloader (DataLoader): The dataloader for training data.\n",
        "        valloader (DataLoader): The dataloader for validation data.\n",
        "        criterion (nn.Module): The loss function.\n",
        "        optimizer (optim.Optimizer): The optimizer.\n",
        "        num_epochs (int): The number of epochs to train.\n",
        "        alpha_threshold (float): The threshold for pruning decision.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[DynamicPruningCNN, List[float], List[float], List[float], List[float], List[int], List[int]]:\n",
        "        A tuple containing the trained model, training losses, training accuracies,\n",
        "        validation losses, validation accuracies, pruned epochs, and pruned layers.\n",
        "    \"\"\"\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "    pruned_epochs, pruned_layers = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"Model parameters:\")\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'gamma' in name:\n",
        "                print(f\"{name}: {param.item():.4f}\")\n",
        "\n",
        "        pbar = tqdm(enumerate(trainloader), total=len(trainloader), desc=f'Training: ')\n",
        "        for i, (inputs, labels) in pbar:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            running_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{running_loss / (i + 1):.4f}',\n",
        "                'accuracy': f'{100 * running_correct / total_samples:.2f}%'\n",
        "            })\n",
        "\n",
        "        train_loss = running_loss / len(trainloader)\n",
        "        train_acc = 100 * running_correct / total_samples\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Check for pruning\n",
        "        new_model, pruned_layer = model.prune_and_rebuild(alpha_threshold)\n",
        "        if pruned_layer is not None:\n",
        "            model = new_model\n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "            pruned_epochs.append(epoch)\n",
        "            pruned_layers.append(pruned_layer)\n",
        "            print(f\"Pruned layer {pruned_layer} at epoch {epoch+1}\")\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate(model, valloader, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1} summary:')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%')\n",
        "\n",
        "        # Unfreeze all layers after one epoch of pruning\n",
        "        if pruned_epochs and epoch == pruned_epochs[-1] + 1:\n",
        "            model.unfreeze_all_layers()\n",
        "            print(\"Unfreezing all layers\")\n",
        "\n",
        "    return model, train_losses, train_accs, val_losses, val_accs, pruned_epochs, pruned_layers"
      ],
      "metadata": {
        "id": "h6pIW3gtiOm2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_metrics(train_losses: List[float], train_accs: List[float],\n",
        "                          val_losses: List[float], val_accs: List[float]) -> None:\n",
        "    \"\"\"\n",
        "    Plots training and validation metrics.\n",
        "\n",
        "    Args:\n",
        "        train_losses (List[float]): Training losses.\n",
        "        train_accs (List[float]): Training accuracies.\n",
        "        val_losses (List[float]): Validation losses.\n",
        "        val_accs (List[float]): Validation accuracies.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train')\n",
        "    plt.plot(val_losses, label='Validation')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss over epochs')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train')\n",
        "    plt.plot(val_accs, label='Validation')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy over epochs')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_validation_metrics.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_gamma_alpha_history(model: DynamicPruningCNN, pruned_epochs: List[int], pruned_layers: List[int]) -> None:\n",
        "    \"\"\"\n",
        "    Plots the history of gamma and alpha values.\n",
        "\n",
        "    Args:\n",
        "        model (DynamicPruningCNN): The trained model.\n",
        "        pruned_epochs (List[int]): List of epochs where pruning occurred.\n",
        "        pruned_layers (List[int]): List of layers that were pruned.\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 16))\n",
        "\n",
        "    for layer_name, gamma_history in model.gamma_history.items():\n",
        "        ax1.plot(gamma_history, label=layer_name)\n",
        "\n",
        "    for layer_name, alpha_history in model.alpha_history.items():\n",
        "        ax2.plot(alpha_history, label=layer_name)\n",
        "\n",
        "    for epoch, layer in zip(pruned_epochs, pruned_layers):\n",
        "        ax1.axvline(x=epoch, color='r', linestyle='--', label=f'Pruned layer {layer}' if layer == pruned_layers[0] else \"\")\n",
        "        ax2.axvline(x=epoch, color='r', linestyle='--', label=f'Pruned layer {layer}' if layer == pruned_layers[0] else \"\")\n",
        "\n",
        "    ax1.set_title('Gamma History')\n",
        "    ax1.set_xlabel('Training Steps')\n",
        "    ax1.set_ylabel('Gamma Value')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.set_title('Alpha History')\n",
        "    ax2.set_xlabel('Training Steps')\n",
        "    ax2.set_ylabel('Alpha Value')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('gamma_alpha_history.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "TWM3H71IipHZ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autograd.set_detect_anomaly(False) # My friend for bug fixing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqlDDip8jigY",
        "outputId": "da64c2ee-f201-4d4b-f505-0308e6c9ced6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7ca1334c7dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "fullset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the dataset\n",
        "train_size = int(0.8 * len(fullset))\n",
        "val_size = len(fullset) - train_size\n",
        "trainset, valset = random_split(fullset, [train_size, val_size])\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model initialization and training\n",
        "model = DynamicPruningCNN(initial_layers=5) # as we'll see, 5 is too much, so maybe we can simplify\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "model, train_losses, train_accs, val_losses, val_accs, pruned_epochs, pruned_layers = train_with_dynamic_pruning(\n",
        "    model, trainloader, valloader, criterion, optimizer, num_epochs=20, alpha_threshold=0.01\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63rwlLeKizx3",
        "outputId": "e9d1ca3d-6e41-4ef9-e6fd-d930ed998c71"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Epoch 1/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.0000\n",
            "layers.1.2.ga_compute.gamma: 0.0000\n",
            "layers.2.2.ga_compute.gamma: 0.0000\n",
            "layers.3.2.ga_compute.gamma: 0.0000\n",
            "layers.4.2.ga_compute.gamma: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:44<00:00, 14.20it/s, loss=1.9523, accuracy=52.37%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 summary:\n",
            "Train Loss: 1.9523, Train Accuracy: 52.37%\n",
            "Val Loss: 1.9251, Val Accuracy: 53.83%\n",
            "\n",
            "Epoch 2/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.2872\n",
            "layers.1.2.ga_compute.gamma: 0.3652\n",
            "layers.2.2.ga_compute.gamma: 0.2950\n",
            "layers.3.2.ga_compute.gamma: 0.3371\n",
            "layers.4.2.ga_compute.gamma: -0.6518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:43<00:00, 14.24it/s, loss=1.8104, accuracy=65.94%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned layer 4 at epoch 2\n",
            "Epoch 2 summary:\n",
            "Train Loss: 1.8104, Train Accuracy: 65.94%\n",
            "Val Loss: 2.3028, Val Accuracy: 10.69%\n",
            "\n",
            "Epoch 3/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.4141\n",
            "layers.1.2.ga_compute.gamma: 0.5302\n",
            "layers.2.2.ga_compute.gamma: 0.4808\n",
            "layers.3.2.ga_compute.gamma: 0.4204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:24<00:00, 25.32it/s, loss=1.8966, accuracy=60.91%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 summary:\n",
            "Train Loss: 1.8966, Train Accuracy: 60.91%\n",
            "Val Loss: 1.8328, Val Accuracy: 65.42%\n",
            "Unfreezing all layers\n",
            "\n",
            "Epoch 4/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.4141\n",
            "layers.1.2.ga_compute.gamma: 0.5302\n",
            "layers.2.2.ga_compute.gamma: 0.4808\n",
            "layers.3.2.ga_compute.gamma: 0.4204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:41<00:00, 14.89it/s, loss=1.8032, accuracy=67.00%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 summary:\n",
            "Train Loss: 1.8032, Train Accuracy: 67.00%\n",
            "Val Loss: 1.8479, Val Accuracy: 61.69%\n",
            "\n",
            "Epoch 5/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.4991\n",
            "layers.1.2.ga_compute.gamma: 0.6294\n",
            "layers.2.2.ga_compute.gamma: 0.5973\n",
            "layers.3.2.ga_compute.gamma: 0.2110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.58it/s, loss=1.7661, accuracy=70.28%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 summary:\n",
            "Train Loss: 1.7661, Train Accuracy: 70.28%\n",
            "Val Loss: 1.7966, Val Accuracy: 66.82%\n",
            "\n",
            "Epoch 6/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5278\n",
            "layers.1.2.ga_compute.gamma: 0.6546\n",
            "layers.2.2.ga_compute.gamma: 0.6277\n",
            "layers.3.2.ga_compute.gamma: 0.0487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.76it/s, loss=1.7450, accuracy=72.25%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 summary:\n",
            "Train Loss: 1.7450, Train Accuracy: 72.25%\n",
            "Val Loss: 1.8053, Val Accuracy: 66.00%\n",
            "\n",
            "Epoch 7/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5366\n",
            "layers.1.2.ga_compute.gamma: 0.6611\n",
            "layers.2.2.ga_compute.gamma: 0.6355\n",
            "layers.3.2.ga_compute.gamma: 0.0135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.71it/s, loss=1.7271, accuracy=73.93%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 summary:\n",
            "Train Loss: 1.7271, Train Accuracy: 73.93%\n",
            "Val Loss: 1.7746, Val Accuracy: 68.92%\n",
            "\n",
            "Epoch 8/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5392\n",
            "layers.1.2.ga_compute.gamma: 0.6625\n",
            "layers.2.2.ga_compute.gamma: 0.6372\n",
            "layers.3.2.ga_compute.gamma: 0.0163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.57it/s, loss=1.7126, accuracy=75.34%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 summary:\n",
            "Train Loss: 1.7126, Train Accuracy: 75.34%\n",
            "Val Loss: 1.7789, Val Accuracy: 68.45%\n",
            "\n",
            "Epoch 9/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5400\n",
            "layers.1.2.ga_compute.gamma: 0.6628\n",
            "layers.2.2.ga_compute.gamma: 0.6376\n",
            "layers.3.2.ga_compute.gamma: -0.0008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.63it/s, loss=1.7009, accuracy=76.58%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 summary:\n",
            "Train Loss: 1.7009, Train Accuracy: 76.58%\n",
            "Val Loss: 1.7559, Val Accuracy: 70.65%\n",
            "\n",
            "Epoch 10/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5403\n",
            "layers.1.2.ga_compute.gamma: 0.6629\n",
            "layers.2.2.ga_compute.gamma: 0.6376\n",
            "layers.3.2.ga_compute.gamma: 0.0029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.69it/s, loss=1.6910, accuracy=77.53%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 summary:\n",
            "Train Loss: 1.6910, Train Accuracy: 77.53%\n",
            "Val Loss: 1.7540, Val Accuracy: 70.81%\n",
            "\n",
            "Epoch 11/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5404\n",
            "layers.1.2.ga_compute.gamma: 0.6629\n",
            "layers.2.2.ga_compute.gamma: 0.6376\n",
            "layers.3.2.ga_compute.gamma: 0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.73it/s, loss=1.6796, accuracy=78.63%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 summary:\n",
            "Train Loss: 1.6796, Train Accuracy: 78.63%\n",
            "Val Loss: 1.7709, Val Accuracy: 68.98%\n",
            "\n",
            "Epoch 12/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5404\n",
            "layers.1.2.ga_compute.gamma: 0.6629\n",
            "layers.2.2.ga_compute.gamma: 0.6376\n",
            "layers.3.2.ga_compute.gamma: -0.0089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.69it/s, loss=1.6714, accuracy=79.42%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 summary:\n",
            "Train Loss: 1.6714, Train Accuracy: 79.42%\n",
            "Val Loss: 1.7746, Val Accuracy: 68.55%\n",
            "\n",
            "Epoch 13/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5404\n",
            "layers.1.2.ga_compute.gamma: 0.6629\n",
            "layers.2.2.ga_compute.gamma: 0.6376\n",
            "layers.3.2.ga_compute.gamma: 0.0020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.79it/s, loss=1.6658, accuracy=79.95%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 summary:\n",
            "Train Loss: 1.6658, Train Accuracy: 79.95%\n",
            "Val Loss: 1.7455, Val Accuracy: 71.69%\n",
            "\n",
            "Epoch 14/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5404\n",
            "layers.1.2.ga_compute.gamma: 0.6629\n",
            "layers.2.2.ga_compute.gamma: 0.6376\n",
            "layers.3.2.ga_compute.gamma: 0.0022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.78it/s, loss=1.6594, accuracy=80.68%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 summary:\n",
            "Train Loss: 1.6594, Train Accuracy: 80.68%\n",
            "Val Loss: 1.7428, Val Accuracy: 71.84%\n",
            "\n",
            "Epoch 15/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5404\n",
            "layers.1.2.ga_compute.gamma: 0.6629\n",
            "layers.2.2.ga_compute.gamma: 0.6376\n",
            "layers.3.2.ga_compute.gamma: -0.0024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.80it/s, loss=1.6479, accuracy=81.90%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 summary:\n",
            "Train Loss: 1.6479, Train Accuracy: 81.90%\n",
            "Val Loss: 1.7367, Val Accuracy: 72.63%\n",
            "\n",
            "Epoch 16/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5404\n",
            "layers.1.2.ga_compute.gamma: 0.6629\n",
            "layers.2.2.ga_compute.gamma: 0.6376\n",
            "layers.3.2.ga_compute.gamma: -0.0067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.73it/s, loss=1.6158, accuracy=85.08%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 summary:\n",
            "Train Loss: 1.6158, Train Accuracy: 85.08%\n",
            "Val Loss: 1.7224, Val Accuracy: 73.93%\n",
            "\n",
            "Epoch 17/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5404\n",
            "layers.1.2.ga_compute.gamma: 0.6629\n",
            "layers.2.2.ga_compute.gamma: 0.6376\n",
            "layers.3.2.ga_compute.gamma: -0.0112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.75it/s, loss=1.6008, accuracy=86.56%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 summary:\n",
            "Train Loss: 1.6008, Train Accuracy: 86.56%\n",
            "Val Loss: 1.7019, Val Accuracy: 76.15%\n",
            "\n",
            "Epoch 18/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5404\n",
            "layers.1.2.ga_compute.gamma: 0.6629\n",
            "layers.2.2.ga_compute.gamma: 0.6376\n",
            "layers.3.2.ga_compute.gamma: -0.0144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.69it/s, loss=1.5907, accuracy=87.55%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 summary:\n",
            "Train Loss: 1.5907, Train Accuracy: 87.55%\n",
            "Val Loss: 1.7158, Val Accuracy: 74.57%\n",
            "\n",
            "Epoch 19/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5404\n",
            "layers.1.2.ga_compute.gamma: 0.6629\n",
            "layers.2.2.ga_compute.gamma: 0.6376\n",
            "layers.3.2.ga_compute.gamma: -0.0080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.72it/s, loss=1.5811, accuracy=88.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 summary:\n",
            "Train Loss: 1.5811, Train Accuracy: 88.50%\n",
            "Val Loss: 1.7122, Val Accuracy: 74.81%\n",
            "\n",
            "Epoch 20/20\n",
            "Model parameters:\n",
            "layers.0.2.ga_compute.gamma: 0.5404\n",
            "layers.1.2.ga_compute.gamma: 0.6629\n",
            "layers.2.2.ga_compute.gamma: 0.6376\n",
            "layers.3.2.ga_compute.gamma: -0.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 625/625 [00:42<00:00, 14.81it/s, loss=1.5746, accuracy=89.22%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 summary:\n",
            "Train Loss: 1.5746, Train Accuracy: 89.22%\n",
            "Val Loss: 1.6974, Val Accuracy: 76.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final test evaluation\n",
        "test_loss, test_acc = evaluate(model, testloader, criterion)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%')\n",
        "\n",
        "# Plot training and validation metrics\n",
        "plot_training_metrics(train_losses, train_accs, val_losses, val_accs)\n",
        "print(\"Training/validation metrics plot saved as 'training_validation_metrics.png'\")\n",
        "\n",
        "# Plot gamma and alpha history\n",
        "plot_gamma_alpha_history(model, pruned_epochs, pruned_layers)\n",
        "print(\"Gamma and Alpha history plot saved as 'gamma_alpha_history.png'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz1zXA66lRxV",
        "outputId": "bb6f8f0a-d8e5-49d8-e1ed-fc208f805f73"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.6984, Test Accuracy: 76.32%\n",
            "Training/validation metrics plot saved as 'training_validation_metrics.png'\n",
            "Gamma and Alpha history plot saved as 'gamma_alpha_history.png'\n"
          ]
        }
      ]
    }
  ]
}